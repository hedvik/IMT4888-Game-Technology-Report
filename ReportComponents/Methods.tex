\section{Methods}
\subsection{Test Cases and Data Acquisition}
Testing was divided into three baseline cases:
\begin{enumerate}
    \item A naive Unity implementation
    \item A job optimised version of the naive Unity implementation
    \item A naive Unity ECS implementation
\end{enumerate}
These three were then further optimised on an individual level to see how far I could get them in terms of performance. In general, the goal for making these test cases was to create a scenario where there are a large amount of moving objects on screen. This is something data-oriented design excels at in theory and it would be interesting to see how big the differences are. 

Data acquisition consisted of a 30 second recording where the frame latencies in milliseconds were collected per frame. This recording was initiated a few seconds after the program had started in order to not include spikes from the start-up. This data was then output to a .CSV file which was uploaded to the project spreadsheet on Google Sheets~\cite{projectSpreadsheet}.

\subsection{Choice of Galaxy Sizes}
The choice of galaxy size for the baseline cases was determined by a few decisions. The minimum size of any galaxy was 10 000 stars as going below this resulted in a somewhat uninteresting visual simulation. Outside of this, the size of the galaxy was largely dependent on overall performance of a test case. The general goal was to have a galaxy at a size that would result in approximately 60 frames per second. The reasoning for this is that taxing the hardware could make the bottlenecks in the software more apparent. This was rather important since I had limited experience with optimisation before going into this project.  

\subsection{Testing Environment}
The testing environment on the hardware side consisted of:
\begin{itemize}
    \item Intel Core i7-6700k CPU
    \item nVidia GeForce GTX 1080 GPU
    \item 16 GB RAM
\end{itemize}

The testing environment on the software side consisted of:
\begin{itemize}
    \item Windows 10 OS - Build 1809
    \item Unity Engine - Version 2018.2.7f1
    \item Unity Entities Package - Version 0.0.12-preview.16
    \item Unity Editor Viewport Resolution of 1047x589
\end{itemize}

\subsection{Determining the Next Course for Optimisation}
I primarily relied on test results and profiler data to determine bottlenecks that could see improvement. This consisted of looking for irregular frame latency spikes and seeing how much time the engine spent on all the elements in a average frame. After determining an area that could use optimisation I started to look up resources for how this could be done before implementing the changes. 

\subsection{Usage of Framerate/Frame Latency Display and In-Editor Testing}
While testing the application, a real-time frame rate and latency display was used in order to get an overview of the general performance~\cite{graphy}. The reason for using a third party display for this is that Unity's own framerate and latency counters are inaccurate. The inaccuracy comes from the fact that the values are inflated to what the engine estimates the program will manage in release mode and not inside the editor~\cite{forumFramerateStats}. This estimation can be incorrect and as such, it has limited value. As an example: Unity's Stats window could for example show that the ECS test case ran at 200 frames per second, while it in reality ran at 68 frames per second. 

Given that ECS is not production ready yet, building the project for release also seemed to actually decrease performance of the ECS cases. This was not directly recorded, but it seemed like the performance dropped by at least 20 frames per second in release mode compared to running in the editor. Due to this, all tests were performed in-editor with the related overhead that this brings. 